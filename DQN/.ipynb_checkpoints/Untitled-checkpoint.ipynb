{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a5127-a859-464e-985c-f37e587cac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import random\n",
    "import string\n",
    "from collections import deque\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "# Utility functions\n",
    "def preprocess(img):\n",
    "    img = img[:84, 6:90]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img\n",
    "\n",
    "def animate(imgs, video_name=None, _return=True):\n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video.write(img)\n",
    "    video.release()\n",
    "    if _return:\n",
    "        from IPython.display import Video\n",
    "        return Video(video_name)\n",
    "\n",
    "def evaluate(agent, n_evals=5):\n",
    "    eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "    eval_env = ImageEnv(eval_env)\n",
    "    scores = 0\n",
    "    for _ in range(n_evals):\n",
    "        (s, _), done, ret = eval_env.reset(), False, 0\n",
    "        while not done:\n",
    "            a = agent.act(s, training=False)\n",
    "            s_prime, r, terminated, truncated, _ = eval_env.step(a)\n",
    "            s = s_prime\n",
    "            ret += r\n",
    "            done = terminated or truncated\n",
    "        scores += ret\n",
    "    return np.round(scores / n_evals, 4)\n",
    "\n",
    "# ImageEnv Wrapper\n",
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip_frames=4, stack_frames=4, initial_no_op=50, **kwargs):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "\n",
    "    def reset(self):\n",
    "        s, info = self.env.reset()\n",
    "        for _ in range(self.initial_no_op):\n",
    "            s, _, terminated, truncated, _ = self.env.step(0)\n",
    "            if terminated or truncated:\n",
    "                s, info = self.env.reset()\n",
    "        s = preprocess(s)\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))\n",
    "        return self.stacked_state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        s = preprocess(s)\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "        return self.stacked_state, reward, terminated, truncated, info\n",
    "\n",
    "# CNNActionValue class\n",
    "class CNNActionValue(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, activation=F.relu):\n",
    "        super(CNNActionValue, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ReplayBuffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e5)):\n",
    "        self.s = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.a = np.zeros((max_size, *action_dim), dtype=np.int64)\n",
    "        self.r = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.s_prime = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def update(self, s, a, r, s_prime, terminated):\n",
    "        self.s[self.ptr] = s\n",
    "        self.a[self.ptr] = a\n",
    "        self.r[self.ptr] = r\n",
    "        self.s_prime[self.ptr] = s_prime\n",
    "        self.terminated[self.ptr] = terminated\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, batch_size)\n",
    "        return (torch.FloatTensor(self.s[ind]), torch.FloatTensor(self.a[ind]), \n",
    "                torch.FloatTensor(self.r[ind]), torch.FloatTensor(self.s_prime[ind]), \n",
    "                torch.FloatTensor(self.terminated[ind]))\n",
    "\n",
    "# DQN class\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.00025, epsilon=1.0, epsilon_min=0.1, gamma=0.99, batch_size=32,\n",
    "                 warmup_steps=5000, buffer_size=int(1e5), target_update_interval=10000):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 1e6\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.optimizer = optim.RMSprop(self.network.parameters(), lr)\n",
    "        self.buffer = ReplayBuffer(state_dim, (1,), buffer_size)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.network.to(self.device)\n",
    "        self.target_network.to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, x, training=True):\n",
    "        self.network.train(training)\n",
    "        if training and (np.random.rand() < self.epsilon or self.total_steps < self.warmup_steps):\n",
    "            return np.random.randint(0, self.action_dim)\n",
    "        x = torch.from_numpy(x).float().unsqueeze(0).to(self.device)\n",
    "        return torch.argmax(self.network(x)).item()\n",
    "\n",
    "    def learn(self):\n",
    "        s, a, r, s_prime, terminated = map(lambda x: x.to(self.device), self.buffer.sample(self.batch_size))\n",
    "        next_q = self.target_network(s_prime).detach()\n",
    "        td_target = r + (1. - terminated) * self.gamma * next_q.max(dim=1, keepdim=True).values\n",
    "        loss = F.mse_loss(self.network(s).gather(1, a.long()), td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return {'total_steps': self.total_steps, 'value_loss': loss.item()}\n",
    "\n",
    "    def process(self, transition):\n",
    "        self.total_steps += 1\n",
    "        self.buffer.update(*transition)\n",
    "        if self.total_steps > self.warmup_steps:\n",
    "            result = self.learn()\n",
    "        if self.total_steps % self.target_update_interval == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay)\n",
    "        return result if self.total_steps > self.warmup_steps else {}\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "state_dim = (4, 84, 84)\n",
    "action_dim = env.action_space.n\n",
    "agent = DQN(state_dim, action_dim)\n",
    "\n",
    "max_steps = 1e6\n",
    "eval_interval = 10000\n",
    "\n",
    "history = {'Step': [], 'AvgReturn': []}\n",
    "\n",
    "(s, _) = env.reset()\n",
    "\n",
    "# Training\n",
    "while agent.total_steps <= max_steps:\n",
    "    a = agent.act(s)\n",
    "    s_prime, r, terminated, truncated, _ = env.step(a)\n",
    "    result = agent.process((s, a, r, s_prime, terminated))\n",
    "    s = s_prime\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "    if agent.total_steps % eval_interval == 0:\n",
    "        ret = evaluate(agent)\n",
    "        history['Step'].append(agent.total_steps)\n",
    "        history['AvgReturn'].append(ret)\n",
    "        clear_output()\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history['Step'], history['AvgReturn'], 'r-')\n",
    "        plt.xlabel('Step', fontsize=16)\n",
    "        plt.ylabel('AvgReturn', fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "        torch.save(agent.network.state_dict(), 'dqn.pt')\n",
    "\n",
    "# Final evaluation and animation\n",
    "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "eval_env = ImageEnv(eval_env)\n",
    "\n",
    "frames = []\n",
    "(s, _), done, ret = eval_env.reset(), False, 0\n",
    "while not done:\n",
    "    frames.append(eval_env.render())\n",
    "    a = agent.act(s, training=False)\n",
    "    s_prime, r, terminated, truncated, _ = eval_env.step(a)\n",
    "    s = s_prime\n",
    "    ret += r\n",
    "    done = terminated or truncated\n",
    "\n",
    "animate(frames, 'animation.webm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817023e-d016-4acc-8439-af38d12714d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import random\n",
    "import string\n",
    "from collections import deque\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "# Utility functions\n",
    "def preprocess(img):\n",
    "    img = img[:84, 6:90]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img\n",
    "\n",
    "def animate(imgs, video_name=None, _return=True):\n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video.write(img)\n",
    "    video.release()\n",
    "    if _return:\n",
    "        from IPython.display import Video\n",
    "        return Video(video_name)\n",
    "\n",
    "def evaluate(agent, n_evals=5):\n",
    "    eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "    eval_env = ImageEnv(eval_env)\n",
    "    scores = 0\n",
    "    for _ in range(n_evals):\n",
    "        (s, _), done, ret = eval_env.reset(), False, 0\n",
    "        while not done:\n",
    "            a = agent.act(s, training=False)\n",
    "            s_prime, r, terminated, truncated, _ = eval_env.step(a)\n",
    "            s = s_prime\n",
    "            ret += r\n",
    "            done = terminated or truncated\n",
    "        scores += ret\n",
    "    return np.round(scores / n_evals, 4)\n",
    "\n",
    "# ImageEnv Wrapper\n",
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip_frames=4, stack_frames=4, initial_no_op=50, **kwargs):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "\n",
    "    def reset(self):\n",
    "        s, info = self.env.reset()\n",
    "        for _ in range(self.initial_no_op):\n",
    "            s, _, terminated, truncated, _ = self.env.step(0)\n",
    "            if terminated or truncated:\n",
    "                s, info = self.env.reset()\n",
    "        s = preprocess(s)\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))\n",
    "        return self.stacked_state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        s = preprocess(s)\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "        return self.stacked_state, reward, terminated, truncated, info\n",
    "\n",
    "# CNNActionValue class\n",
    "class CNNActionValue(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, activation=F.relu):\n",
    "        super(CNNActionValue, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ReplayBuffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e5)):\n",
    "        self.s = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.a = np.zeros((max_size, *action_dim), dtype=np.int64)\n",
    "        self.r = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.s_prime = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def update(self, s, a, r, s_prime, terminated):\n",
    "        self.s[self.ptr] = s\n",
    "        self.a[self.ptr] = a\n",
    "        self.r[self.ptr] = r\n",
    "        self.s_prime[self.ptr] = s_prime\n",
    "        self.terminated[self.ptr] = terminated\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, batch_size)\n",
    "        return (torch.FloatTensor(self.s[ind]), torch.FloatTensor(self.a[ind]), \n",
    "                torch.FloatTensor(self.r[ind]), torch.FloatTensor(self.s_prime[ind]), \n",
    "                torch.FloatTensor(self.terminated[ind]))\n",
    "\n",
    "# DQN class\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.00025, epsilon=1.0, epsilon_min=0.1, gamma=0.99, batch_size=32,\n",
    "                 warmup_steps=5000, buffer_size=int(1e5), target_update_interval=10000):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 1e6\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.buffer = ReplayBuffer(state_dim, [1], buffer_size)\n",
    "        self.network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.network.to(self.device)\n",
    "        self.target_network.to(self.device)\n",
    "        self.steps = 0\n",
    "\n",
    "    def act(self, s, training=True):\n",
    "        if training and np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        s = torch.FloatTensor(s).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.network(s).argmax().item()\n",
    "\n",
    "    def update(self):\n",
    "        if self.steps < self.warmup_steps:\n",
    "            return\n",
    "        s, a, r, s_prime, terminated = self.buffer.sample(self.batch_size)\n",
    "        s, a, r, s_prime, terminated = s.to(self.device), a.to(self.device), r.to(self.device), s_prime.to(self.device), terminated.to(self.device)\n",
    "        q_values = self.network(s).gather(1, a.long())\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(s_prime).max(1, keepdim=True)[0]\n",
    "            target = r + self.gamma * next_q_values * (1 - terminated)\n",
    "        loss = self.loss(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_interval == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "# Training loop\n",
    "env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "env = ImageEnv(env)\n",
    "\n",
    "state_dim = [4, 84, 84]\n",
    "action_dim = 5\n",
    "lr = 0.00025\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "warmup_steps = 5000\n",
    "buffer_size = int(1e5)\n",
    "target_update_interval = 10000\n",
    "n_episodes = 1000\n",
    "max_steps = 1000\n",
    "\n",
    "agent = DQN(state_dim, action_dim, lr, epsilon, epsilon_min, gamma, batch_size, warmup_steps, buffer_size, target_update_interval)\n",
    "\n",
    "history = {'Step': [], 'AvgReturn': []}\n",
    "eval_interval = 10\n",
    "returns = deque(maxlen=100)\n",
    "\n",
    "for episode in range(1, n_episodes + 1):\n",
    "    (s, _), done, ret = env.reset(), False, 0\n",
    "    for step in range(max_steps):\n",
    "        a = agent.act(s)\n",
    "        s_prime, r, terminated, truncated, _ = env.step(a)\n",
    "        done = terminated or truncated\n",
    "        agent.buffer.update(s, a, r, s_prime, terminated)\n",
    "        agent.update()\n",
    "        s = s_prime\n",
    "        ret += r\n",
    "        if done:\n",
    "            break\n",
    "    returns.append(ret)\n",
    "    if episode % eval_interval == 0:\n",
    "        avg_return = np.mean(returns)\n",
    "        history['Step'].append(agent.steps)\n",
    "        history['AvgReturn'].append(avg_return)\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(history['Step'], history['AvgReturn'], 'r-')\n",
    "        plt.xlabel('Step', fontsize=16)\n",
    "        plt.ylabel('AvgReturn', fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "        torch.save(agent.network.state_dict(), 'dqn.pt')\n",
    "\n",
    "# Final evaluation and animation\n",
    "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "eval_env = ImageEnv(eval_env)\n",
    "\n",
    "frames = []\n",
    "(s, _), done, ret = eval_env.reset(), False, 0\n",
    "while not done:\n",
    "    frames.append(eval_env.render())\n",
    "    a = agent.act(s, training=False)\n",
    "    s_prime, r, terminated, truncated, _ = eval_env.step(a)\n",
    "    s = s_prime\n",
    "    ret += r\n",
    "    done = terminated or truncated\n",
    "\n",
    "animate(frames, 'animation.webm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808b0f1-bcfc-4ac1-addf-4670f8bf4009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f905d3-f6cc-4d01-931c-70c2f59c8b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78d661-141e-4c76-8e91-2c189cfa8dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "import random\n",
    "import string\n",
    "from collections import deque\n",
    "from IPython.display import HTML, clear_output\n",
    "\n",
    "# Utility functions\n",
    "def preprocess(img):\n",
    "    img = img[:84, 6:90]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img\n",
    "\n",
    "def animate(imgs, video_name=None, _return=True):\n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video.write(img)\n",
    "    video.release()\n",
    "    if _return:\n",
    "        from IPython.display import Video\n",
    "        return Video(video_name)\n",
    "\n",
    "def evaluate(agent, n_evals=5):\n",
    "    eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "    eval_env = ImageEnv(eval_env)\n",
    "    scores = 0\n",
    "    all_frames = []\n",
    "    for _ in range(n_evals):\n",
    "        (s, _), done, ret = eval_env.reset(), False, 0\n",
    "        frames = []\n",
    "        while not done:\n",
    "            a = agent.act(s, training=False)\n",
    "            s_prime, r, terminated, truncated, _ = eval_env.step(a)\n",
    "            frames.append(eval_env.render())\n",
    "            s = s_prime\n",
    "            ret += r\n",
    "            done = terminated or truncated\n",
    "        scores += ret\n",
    "        all_frames.append(frames)\n",
    "    avg_score = np.round(scores / n_evals, 4)\n",
    "    return avg_score, all_frames\n",
    "\n",
    "# ImageEnv Wrapper\n",
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip_frames=4, stack_frames=4, initial_no_op=50, **kwargs):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "\n",
    "    def reset(self):\n",
    "        s, info = self.env.reset()\n",
    "        for _ in range(self.initial_no_op):\n",
    "            s, _, terminated, truncated, _ = self.env.step(0)\n",
    "            if terminated or truncated:\n",
    "                s, info = self.env.reset()\n",
    "        s = preprocess(s)\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))\n",
    "        return self.stacked_state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        s = preprocess(s)\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "        return self.stacked_state, reward, terminated, truncated, info\n",
    "\n",
    "# CNNActionValue class\n",
    "class CNNActionValue(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, activation=F.relu):\n",
    "        super(CNNActionValue, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ReplayBuffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e5)):\n",
    "        self.s = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.a = np.zeros((max_size, *action_dim), dtype=np.int64)\n",
    "        self.r = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.s_prime = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def update(self, s, a, r, s_prime, terminated):\n",
    "        self.s[self.ptr] = s\n",
    "        self.a[self.ptr] = a\n",
    "        self.r[self.ptr] = r\n",
    "        self.s_prime[self.ptr] = s_prime\n",
    "        self.terminated[self.ptr] = terminated\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, batch_size)\n",
    "        return (torch.FloatTensor(self.s[ind]), torch.FloatTensor(self.a[ind]), \n",
    "                torch.FloatTensor(self.r[ind]), torch.FloatTensor(self.s_prime[ind]), \n",
    "                torch.FloatTensor(self.terminated[ind]))\n",
    "\n",
    "# DQN class\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.00025, epsilon=1.0, epsilon_min=0.1, gamma=0.99, batch_size=32,\n",
    "                 warmup_steps=5000, buffer_size=int(1e5), target_update_interval=10000):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 1e6\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.buffer = ReplayBuffer(state_dim, [1], buffer_size)\n",
    "        self.network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.network.to(self.device)\n",
    "        self.target_network.to(self.device)\n",
    "        self.steps = 0\n",
    "\n",
    "    def act(self, s, training=True):\n",
    "        if training and np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        s = torch.FloatTensor(s).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.network(s).argmax().item()\n",
    "\n",
    "    def update(self):\n",
    "        if self.steps < self.warmup_steps:\n",
    "            return\n",
    "        s, a, r, s_prime, terminated = self.buffer.sample(self.batch_size)\n",
    "        s, a, r, s_prime, terminated = s.to(self.device), a.to(self.device), r.to(self.device), s_prime.to(self.device), terminated.to(self.device)\n",
    "        q_values = self.network(s).gather(1, a.long())\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(s_prime).max(1, keepdim=True)[0]\n",
    "            target = r + self.gamma * next_q_values * (1 - terminated)\n",
    "        loss = self.loss(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_interval == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'network_state_dict': self.network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'steps': self.steps,\n",
    "            'epsilon': self.epsilon,\n",
    "            'buffer': self.buffer,\n",
    "        }, filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath)\n",
    "        self.network.load_state_dict(checkpoint['network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.steps = checkpoint['steps']\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.buffer = checkpoint['buffer']\n",
    "\n",
    "# Training loop\n",
    "env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "env = ImageEnv(env)\n",
    "\n",
    "state_dim = [4, 84, 84]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "agent = DQN(state_dim, action_dim)\n",
    "\n",
    "# Load the model if exists\n",
    "load_model = True\n",
    "model_path = 'dqn_car_racing.pth'\n",
    "if load_model:\n",
    "    try:\n",
    "        agent.load(model_path)\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No model found at {model_path}, starting from scratch\")\n",
    "\n",
    "n_episodes = 1000\n",
    "save_interval = 10\n",
    "eval_interval = 50\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    (s, _), done, ret = env.reset(), False, 0\n",
    "    while not done:\n",
    "        a = agent.act(s)\n",
    "        s_prime, r, terminated, truncated, _ = env.step(a)\n",
    "        agent.buffer.update(s, a, r, s_prime, terminated)\n",
    "        agent.update()\n",
    "        s = s_prime\n",
    "        ret += r\n",
    "        done = terminated or truncated\n",
    "    episode_rewards.append(ret)\n",
    "    print(f\"Episode {episode + 1}: {ret}\")\n",
    "\n",
    "    # Save the model at intervals\n",
    "    if (episode + 1) % save_interval == 0:\n",
    "        agent.save(model_path)\n",
    "        print(f\"Model saved at episode {episode + 1}\")\n",
    "\n",
    "    # Evaluate the model at intervals and save animation\n",
    "    if (episode + 1) % eval_interval == 0:\n",
    "        avg_score, all_frames = evaluate(agent)\n",
    "        print(f\"Evaluation at episode {episode + 1}: Avg. score: {avg_score}\")\n",
    "        for idx, frames in enumerate(all_frames):\n",
    "            animate(frames, f'eval_{episode + 1}_{idx + 1}.webm', _return=False)\n",
    "\n",
    "        # Plot training chart\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(episode_rewards, label='Episode Reward')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.title('Training Progress')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a34119-1718-4fdb-805c-4950c6465509",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forklift",
   "language": "python",
   "name": "forklift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
