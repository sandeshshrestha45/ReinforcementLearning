{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a98925-978c-4d28-a273-9274de25856e",
   "metadata": {},
   "source": [
    "# DQN Implementation\n",
    "\n",
    "### Overview\n",
    "The paper introduces the Deep Q-Network (DQN) algorithm, which combines Q-learning with deep neural networks. The key contributions of the paper are:\n",
    "\n",
    "1. Q-Learning with Function Approximation: Using a convolutional neural network (CNN) to approximate the Q-function, which is a measure of the expected future rewards given a state-action pair.\n",
    "2. Experience Replay: Storing the agent's experiences in a replay buffer and sampling random mini-batches from it to break the correlation between consecutive samples.\n",
    "3. Fixed Q-Target: Using a separate target network to generate stable target Q-values, which are updated less frequently than the main Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd204d60-ec7f-4f18-84e8-fe5ec90a5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML, clear_output\n",
    "import cv2\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76391e28-384b-4a05-864b-113a364ad6a0",
   "metadata": {},
   "source": [
    "### Preprocess Function\n",
    "\n",
    "From DQN paper,\n",
    "*The raw frames are preprocessed by first converting their RGB representation to gray-scale\n",
    "…\n",
    "The final input representation is obtained by cropping an 84 × 84 region of the image that roughly captures the playing area.*\n",
    "* preprocess: Converts the input image to grayscale and normalizes it. This reduces the complexity of the input and speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3bc826-da9c-445d-98cb-baa380677cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    img = img[:84, 6:90]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84cce44-417f-4679-be71-c0da75abda3c",
   "metadata": {},
   "source": [
    "### Animate Function\n",
    "* animate: Creates a video from a sequence of images and saves it. If _return is True, it returns an IPython Video object for display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80463948-09ac-4926-8eb7-3a119f1bb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(imgs, video_name, _return=True):\n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video.write(img)\n",
    "    video.release()\n",
    "    if _return:\n",
    "        from IPython.display import Video\n",
    "        return Video(video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40494b-e421-4693-9925-a94d2c57388a",
   "metadata": {},
   "source": [
    "### Evaluate Function\n",
    "* evaluate: Evaluates the agent's performance over multiple episodes and returns the average score. This helps in tracking the learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99d1c9-b1d6-452d-8a18-ec4aa7692c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, n_evals=5):\n",
    "    eval_env = gym.make('CarRacing-v2', continuous=False)\n",
    "    eval_env = ImageEnv(eval_env)\n",
    "    scores = 0\n",
    "    for _ in range(n_evals):\n",
    "        (s, _), done, ret = eval_env.reset(), False, 0\n",
    "        while not done:\n",
    "            a = agent.act(s, training=False)\n",
    "            s_prime, r, terminated, truncated, _ = eval_env.step(a)\n",
    "            s = s_prime\n",
    "            ret += r\n",
    "            done = terminated or truncated\n",
    "        scores += ret\n",
    "    return np.round(scores / n_evals, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757206ef-afaf-4260-b16c-77206d1df06b",
   "metadata": {},
   "source": [
    "# Image Environment Wrapper\n",
    "A wrapper around the environment that:\n",
    "* reset: Resets the environment, performs no-ops, preprocesses the state, and stacks frames.\n",
    "* step: Executes the action, skips frames, preprocesses the state, and stacks frames. <br>\n",
    "\n",
    "*For the experiments in this paper, the function $\\phi$\n",
    " from algorithm 1 applies this preprocessing to the last 4 frames of a history and stacks them to produce the input to the Q-function <br>\n",
    "… <br>\n",
    "we also use a simple frame-skipping technique [3]. More precisely, the agent sees and selects actions on every kth frame instead of every frame, and its last action is repeated on skipped frames.* <br>\n",
    "\n",
    "#### Key Points\n",
    "* Skipping Frames: By skipping frames, the environment's observations are updated only every skip_frames steps, which can reduce computational load and make training more efficient.\n",
    "* Stacking Frames: Multiple frames are stacked together to form a single state. This helps the agent to understand the temporal aspect of the environment.\n",
    "* Initial No-op Actions: A number of no-op actions are performed initially to randomize the starting state.\n",
    "* Preprocessing: The observations are preprocessed before being used. This can include resizing, normalization, etc., to make the data suitable for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed61558-0a0c-4268-aa34-5f90520361e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageEnv Wrapper\n",
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip_frames=4, stack_frames=4, initial_no_op=50, **kwargs):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "\n",
    "    def reset(self):\n",
    "        s, info = self.env.reset()\n",
    "        for _ in range(self.initial_no_op):\n",
    "            s, _, terminated, truncated, _ = self.env.step(0)\n",
    "            if terminated or truncated:\n",
    "                s, info = self.env.reset()\n",
    "        s = preprocess(s)\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))\n",
    "        return self.stacked_state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        s = preprocess(s)\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "        return self.stacked_state, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9545249d-f93d-4546-9e78-a8390c98f518",
   "metadata": {},
   "source": [
    "# The Q - Network\n",
    "From DQN paper, \n",
    "*The input to the neural network consists is an 84 × 84 × 4 image produced by $\\phi$\n",
    ". The first hidden layer convolves 32 8 × 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 64 4 × 4 filters with stride 2, and the third hidden layer consists of 64 3x3 filters with stride 1 again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action.* \n",
    "* CNNActionValue: A convolutional neural network for approximating the Q-function.\n",
    "    * forward: Defines the forward pass of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a1aea-eae7-4215-9c34-d46e40d32e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNActionValue class\n",
    "class CNNActionValue(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, activation=F.relu):\n",
    "        super(CNNActionValue, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, action_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.activation(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb1c75-f8d3-468c-be8a-024d004450a4",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "This code defines a ReplayBuffer class, which is used in reinforcement learning (RL) to store and sample experiences. \n",
    "\n",
    "### Class Initialization (__init__ method)\n",
    "##### Parameters:\n",
    "\n",
    "* state_dim: The dimensions of the state space.\n",
    "* action_dim: The dimensions of the action space.\n",
    "* max_size: The maximum size of the replay buffer, default is 100,000.\n",
    "\n",
    "##### Attributes:\n",
    "\n",
    "* self.s: A numpy array to store states with shape (max_size, *state_dim).\n",
    "* self.a: A numpy array to store actions with shape (max_size, *action_dim).\n",
    "* self.r: A numpy array to store rewards with shape (max_size, 1).\n",
    "* self.s_prime: A numpy array to store next states with shape (max_size, *state_dim).\n",
    "* self.terminated: A numpy array to store terminal flags (indicating episode termination) with shape (max_size, 1).\n",
    "* self.ptr: A pointer to indicate the current position to insert the new experience.\n",
    "* self.size: The current size of the buffer (number of stored experiences).\n",
    "* self.max_size: The maximum capacity of the buffer.\n",
    "\n",
    "### Updating the Buffer (update method)\n",
    "\n",
    "##### Parameters:\n",
    "\n",
    "* s: The current state.\n",
    "* a: The action taken.\n",
    "* r: The reward received.\n",
    "* s_prime: The next state.\n",
    "* terminated: A flag indicating whether the episode has terminated.\n",
    "##### Functionality:\n",
    "\n",
    "* Store the experience (s, a, r, s_prime, terminated) at the current position indicated by self.ptr.\n",
    "* Increment the pointer (self.ptr) and wrap around if it exceeds max_size (circular buffer).\n",
    "* Update the size of the buffer (self.size) ensuring it doesn't exceed max_size.\n",
    "\n",
    "### Sampling from the Buffer (sample method)\n",
    "##### Parameter:\n",
    "\n",
    "* batch_size: The number of experiences to sample.\n",
    "\n",
    "##### Functionality:\n",
    "\n",
    "* Randomly sample batch_size indices from the range (0, self.size).\n",
    "* Retrieve the experiences (states, actions, rewards, next states, and terminal flags) corresponding to these indices.\n",
    "* Convert the sampled experiences to PyTorch tensors for use in training neural networks.\n",
    "\n",
    "Replay Buffer stores the agent's experiences and allows for efficient sampling of random mini-batches, which is essential for breaking the correlation between consecutive experiences and stabilizing training. <br>\n",
    "Reference:  https://github.com/sfujim/TD3/blob/master/utils.py#L5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70605e15-4112-46e1-81ca-5827fb2b985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReplayBuffer class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e5)):\n",
    "        self.s = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.a = np.zeros((max_size, *action_dim), dtype=np.int64)\n",
    "        self.r = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.s_prime = np.zeros((max_size, *state_dim), dtype=np.float32)\n",
    "        self.terminated = np.zeros((max_size, 1), dtype=np.float32)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def update(self, s, a, r, s_prime, terminated):\n",
    "        self.s[self.ptr] = s\n",
    "        self.a[self.ptr] = a\n",
    "        self.r[self.ptr] = r\n",
    "        self.s_prime[self.ptr] = s_prime\n",
    "        self.terminated[self.ptr] = terminated\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, batch_size)\n",
    "        return (torch.FloatTensor(self.s[ind]), torch.FloatTensor(self.a[ind]), \n",
    "                torch.FloatTensor(self.r[ind]), torch.FloatTensor(self.s_prime[ind]), \n",
    "                torch.FloatTensor(self.terminated[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9971b0-0a0e-4439-bca0-731d8d2110e5",
   "metadata": {},
   "source": [
    "# DQN Class\n",
    "\n",
    "The DQN class encapsulates the components and functionalities required for a DQN agent. <br>\n",
    "\n",
    "### Initialization (__init__ method) \n",
    "The constructor initializes various parameters and objects needed for the DQN agent.\n",
    "\n",
    "##### Parameters:\n",
    "* state_dim: Dimensions of the state space.\n",
    "* action_dim: Number of possible actions.\n",
    "* lr: Learning rate for the optimizer. Default is 0.00025.\n",
    "* epsilon: Initial exploration rate. Default is 1.0.\n",
    "* epsilon_min: Minimum exploration rate. Default is 0.1.\n",
    "* gamma: Discount factor for future rewards. Default is 0.99.\n",
    "* batch_size: Number of experiences to sample for each learning step. Default is 32.\n",
    "* warmup_steps: Number of steps to collect experiences before starting the learning process. Default is 5000.\n",
    "* buffer_size: Maximum size of the replay buffer. Default is 100000.\n",
    "* target_update_interval: Number of steps between updates of the target network. Default is 10000.\n",
    "\n",
    "##### Neural Networks:\n",
    "* self.network: Main Q-network used for action selection.\n",
    "* self.target_network: Target Q-network used for stable Q-value estimation.<br>\n",
    "Both networks are instances of CNNActionValue and have the same architecture.\n",
    "\n",
    "##### Action Selection (act method)\n",
    "* Selects an action based on the current policy.\n",
    "* Uses ε-greedy strategy: with probability epsilon, selects a random action; otherwise, selects the action with the highest Q-value predicted by the network.\n",
    "* Converts the state to a PyTorch tensor, passes it through the network, and selects the action with the highest Q-value.\n",
    "\n",
    "##### Learning (learn method)\n",
    "* Samples a batch of experiences from the replay buffer.\n",
    "* Computes the target Q-values using the target network.\n",
    "* Computes the loss between the predicted Q-values (from the main network) and the target Q-values.\n",
    "* Performs a gradient descent step to minimize the loss.\n",
    "\n",
    "##### Processing Transitions (process method)\n",
    "* Updates the total number of steps.\n",
    "* Adds the new transition to the replay buffer.\n",
    "* If the total steps exceed the warmup steps, performs a learning step.\n",
    "* Periodically updates the target network.\n",
    "* Adjusts the epsilon value according to the epsilon decay schedule.\n",
    "\n",
    "\n",
    "Initialization: Sets up the main and target networks, optimizer, replay buffer, and various hyperparameters.<br>\n",
    "Action Selection: Uses ε-greedy strategy to balance exploration and exploitation.<br>\n",
    "Learning: Samples experiences, computes loss, and updates the main network.<br>\n",
    "Processing Transitions: Manages transitions, triggers learning, updates target network, and decays epsilon.<br><br>\n",
    "This structure helps the DQN agent learn effective policies by leveraging experience replay and target networks to stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f606f-a779-484c-a364-d08592e0c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN class\n",
    "class DQN:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.00025, epsilon=1.0, epsilon_min=0.1, gamma=0.99, batch_size=32,\n",
    "                 warmup_steps=5000, buffer_size=int(1e5), target_update_interval=10000):\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = (epsilon - epsilon_min) / 1e6\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network = CNNActionValue(state_dim[0], action_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.optimizer = torch.optim.RMSprop(self.network.parameters(), lr)\n",
    "        self.buffer = ReplayBuffer(state_dim, (1,), buffer_size)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.network.to(self.device)\n",
    "        self.target_network.to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, x, training=True):\n",
    "        self.network.train(training)\n",
    "        if training and (np.random.rand() < self.epsilon or self.total_steps < self.warmup_steps):\n",
    "            return np.random.randint(0, self.action_dim)\n",
    "        x = torch.from_numpy(x).float().unsqueeze(0).to(self.device)\n",
    "        return torch.argmax(self.network(x)).item()\n",
    "\n",
    "    def learn(self):\n",
    "        s, a, r, s_prime, terminated = map(lambda x: x.to(self.device), self.buffer.sample(self.batch_size))\n",
    "        next_q = self.target_network(s_prime).detach()\n",
    "        td_target = r + (1. - terminated) * self.gamma * next_q.max(dim=1, keepdim=True).values\n",
    "        loss = F.mse_loss(self.network(s).gather(1, a.long()), td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return {'total_steps': self.total_steps, 'value_loss': loss.item()}\n",
    "\n",
    "    def process(self, transition):\n",
    "        self.total_steps += 1\n",
    "        self.buffer.update(*transition)\n",
    "        if self.total_steps > self.warmup_steps:\n",
    "            result = self.learn()\n",
    "        if self.total_steps % self.target_update_interval == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay)\n",
    "        return result if self.total_steps > self.warmup_steps else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be53d1-5e89-4d24-b432-1c28762b4261",
   "metadata": {},
   "source": [
    "# Main Training Loop\n",
    "### Initialization:\n",
    "* Creates the environment and wraps it with ImageEnv.\n",
    "* Defines the state and action dimensions.\n",
    "* Initializes the DQN agent.\n",
    "* Sets the maximum steps and evaluation interval.\n",
    "* Initializes a dictionary to store the training history.\n",
    "* Resets the environment and gets the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf26dd7-0f8b-4274-a8b9-bb7e46a144d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "state_dim = (4, 84, 84)\n",
    "action_dim = env.action_space.n\n",
    "agent = DQN(state_dim, action_dim)\n",
    "\n",
    "max_steps = 1e6 #The maximum number of training steps.\n",
    "eval_interval = 10000 #The number of steps between evaluations.\n",
    "\n",
    "history = {'Step': [], 'AvgReturn': []}\n",
    "\n",
    "(s, _) = env.reset() #The environment is reset, and the initial state is stored in s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df679271-8fa6-4f0d-aa68-c5293abacb71",
   "metadata": {},
   "source": [
    "### Training Loop:\n",
    "* The loop runs until the total steps exceed max_steps.\n",
    "* The agent takes an action based on the current state.\n",
    "* Steps the environment with the action, receiving the next state, reward, and done flags.\n",
    "* Processes the transition.\n",
    "* Updates the current state.\n",
    "* If the episode ends (terminated or truncated), resets the environment.\n",
    "* Every eval_interval steps, evaluates the agent, logs the results, plots the training progress, and saves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4fbca-b6bb-40a7-bf51-9e0e7c918b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "while agent.total_steps <= max_steps:\n",
    "    a = agent.act(s) # The agent selects an action a using its act method.\n",
    "    s_prime, r, terminated, truncated, _ = env.step(a) #The environment performs the action and returns the next state s_prime, reward r, and termination flags terminated and truncated.\n",
    "    result = agent.process((s, a, r, s_prime, terminated)) #The agent processes the transition (s, a, r, s_prime, terminated) using the process method.\n",
    "    s = s_prime #The next state s_prime becomes the current state s.\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset() #If the episode ends (terminated or truncated), the environment is reset.\n",
    "    #Every eval_interval steps, the agent is evaluated using the evaluate function. The results are recorded in history, and a plot is updated and displayed. The agent's network is saved to a file dqn-CarRacing.pt.\n",
    "    if agent.total_steps % eval_interval == 0:\n",
    "        ret = evaluate(agent)\n",
    "        history['Step'].append(agent.total_steps)\n",
    "        history['AvgReturn'].append(ret)\n",
    "        clear_output()\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history['Step'], history['AvgReturn'], 'b-')\n",
    "        plt.xlabel('Step', fontsize=16)\n",
    "        plt.ylabel('AvgReturn', fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "        torch.save(agent.network.state_dict(), 'dqn-CarRacing.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2773e85-b79b-469f-a0fb-9d5473e6b142",
   "metadata": {},
   "source": [
    "### Final Evaluation and Animation\n",
    "* Creates a new evaluation environment with rendering enabled.\n",
    "* Runs an evaluation episode, rendering each frame.\n",
    "* Stores each rendered frame.\n",
    "* Uses the agent to take actions without exploration.\n",
    "* Animates the frames and saves the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfbe27-dfc3-4686-a413-b500e2525079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation and animation\n",
    "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array') #A new evaluation environment eval_env is created with render_mode='rgb_array' to capture frames for animation.\n",
    "eval_env = ImageEnv(eval_env) #The environment is wrapped with ImageEnv.\n",
    "\n",
    "frames = [] #A list to store rendered frames.\n",
    "(s, _), done, ret = eval_env.reset(), False, 0 #The environment is reset, and initial values are set for done and ret.\n",
    "while not done: #The loop continues until the episode ends (done is True).\n",
    "    frames.append(eval_env.render()) #The current frame is rendered and added to frames.\n",
    "    a = agent.act(s, training=False) #The agent selects an action a without exploration (training=False).\n",
    "    s_prime, r, terminated, truncated, _ = eval_env.step(a) #The environment performs the action and returns the next state s_prime, reward r, and termination flags.\n",
    "    s = s_prime #The next state s_prime becomes the current state s.\n",
    "    ret += r #The reward r is added to the cumulative return ret.\n",
    "    done = terminated or truncated # The loop ends if the episode is terminated or truncated.\n",
    "\n",
    "animate(frames, 'animation.webm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da077b51-4e1b-47bc-8a84-acb38f1cde68",
   "metadata": {},
   "source": [
    "##### Load Model, Continue Training or Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc366f-d195-4d0e-8712-d0e65d9e0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model function\n",
    "def load_model(agent, model_path): #This function loads a pre-trained model into the agent.\n",
    "    agent.network.load_state_dict(torch.load(model_path)) #Loads the state dictionary (weights) from model_path into agent.network.\n",
    "    agent.target_network.load_state_dict(agent.network.state_dict()) #Copies the loaded weights from agent.network to agent.target_network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634139e-c562-4b61-9ec0-4de3a5da42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "env = gym.make('CarRacing-v2', continuous=False)\n",
    "env = ImageEnv(env)\n",
    "\n",
    "state_dim = (4, 84, 84)\n",
    "action_dim = env.action_space.n\n",
    "agent = DQN(state_dim, action_dim)\n",
    "\n",
    "# Load the model if it exists\n",
    "model_path = 'dqn-CarRacing.pt'\n",
    "try:\n",
    "    load_model(agent, model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Model file not found. Training from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b2244-c754-4ac5-97cd-d0a51c43bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training\n",
    "max_steps = 1e6\n",
    "eval_interval = 10000\n",
    "\n",
    "history = {'Step': [], 'AvgReturn': []}\n",
    "\n",
    "(s, _) = env.reset()\n",
    "while True:\n",
    "    a = agent.act(s)\n",
    "    s_prime, r, terminated, truncated, _ = env.step(a)\n",
    "    result = agent.process((s, a, r, s_prime, terminated))\n",
    "    s = s_prime\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "\n",
    "    if agent.total_steps % eval_interval == 0:\n",
    "        ret = evaluate(agent)\n",
    "        history['Step'].append(agent.total_steps)\n",
    "        history['AvgReturn'].append(ret)\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(history['Step'], history['AvgReturn'], 'b-')\n",
    "        plt.xlabel('Step', fontsize=16)\n",
    "        plt.ylabel('AvgReturn', fontsize=16)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "        torch.save(agent.network.state_dict(), model_path)\n",
    "\n",
    "    if agent.total_steps > max_steps:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a5651-eb85-4f54-a8f0-78e75f65e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation and animation\n",
    "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "eval_env = ImageEnv(eval_env)\n",
    "\n",
    "frames = []\n",
    "(s, _), done, ret = eval_env.reset(), False, 0\n",
    "while not done:\n",
    "    frames.append(eval_env.render())\n",
    "    a = agent.act(s, training=False)\n",
    "    s_prime, r, terminated, truncated, _ = eval_env.step(a)\n",
    "    s = s_prime\n",
    "    ret += r\n",
    "    done = terminated or truncated\n",
    "\n",
    "animate(frames, 'animation.webm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forklift",
   "language": "python",
   "name": "forklift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
